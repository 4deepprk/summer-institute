---
title: 'Non-probability-based Surveys in Practice'
author: "Matthew Salganik & Yo-Yo Chen"
date: "Summer Institute in Computational Social Science, 6/22/2017"
output: pdf_document
urlcolor: blue
---

How accurate are estimates from non-probability-based online surveys? Can results from non-probability-based online surveys generalize to the broader population? In this activity, we will administer a survey on MTurk and try a variety of estimation techniques.  Then, we will compare our estimates to those that come from high-quality probability-based samples that were conducted by the Pew Research Center. 

In the process, we will gain experience with writing survey questionnaires, creating surveys on Qualtrics, publishing surveys on MTurk, implementing various weighting methods, documenting surveys for dissemination, and thinking about broader issues related to survey errors and data quality.  Please read the entire activity before beginning.

## Before the group activity:
* Read Chapter 3 in *Bit by Bit*
* Read the paper that motivated this activity: [*Online, Opt-in Surveys: Fast and Cheap, but are they Accurate?*](https://5harad.com/papers/dirtysurveys.pdf) by Goel et al.  In this paper, the researchers collect an opt-in, non-probability survey of people on MTurk.  They compared the raw sample estimates and those made after various weighting adjustments to estimate that come from surveys using probability-based sampling methods.
* In order to complete this activity, it will be helpful for you to learn/review basic weighting methods.  Therefore, please read [*Weighting methods*](http://hbanaszak.mjr.uw.edu.pl/TempTxt/KaltonFlores-Cervantes_2003_Weighting%20Methods.pdf) by Kalton and Flores-Cervantes.  Please focus on cell weighting and raking, although you are welcome to try to use more advanced methods if you wish.
* Complete a quick online poll [here](https://doodle.com/poll/ip4ycg8zz4d99ecd) to select the questions we will collect in our survey. Select a total of 12 questions from two recent Pew surveys: select 6 questions from a survey on [*political attitudes*](http://assets.pewresearch.org/wp-content/uploads/sites/5/2017/04/17084414/04-17-17-Political-topline-for-release.pdf), including 3 questions for which you think the weighted estimates from MTurk and the Pew estimates will be close 3 questions for which they won't be close; using the same criteria, select another 6 questions from a Pew survey on [*technology use*](http://assets.pewresearch.org/wp-content/uploads/sites/14/2017/05/15170020/PI_2017.05.17_Older-Americans-Tech_TOPLINE.pdf)

## During group activity
* Create a survey on Qualtrics, including the 12 selected questions.

    - Don’t forget to include demographic questions you will need later for post-stratification. You are provided with a [file of cross-tabulations](https://github.com/compsocialscience/summer-institute/blob/master/2017/materials/day4_surveys/population.csv) by key demographic variables from the Census Bureau. These population estimates are collected with questions from the [American Community Survey](https://www2.census.gov/programs-surveys/acs/methodology/questionnaires/2017/quest17.pdf), which you can use in your own survey. 
    - In designing your survey, think of ways to ensure the quality of the data collected. For example, randomize the question order, use attention screener, treating "don't know"s carefully etc.  This should go into your documentation.
    - Because we will later combine the data collected from each group, please number the questions in the same way using the original Pew question numbering given ([How to change question numbering on Qualtrics](https://www.qualtrics.com/support/survey-platform/survey-module/survey-tools/general-tools/auto-number-questions/)) 
    - Make sure you pilot test the survey (using the preview function on Qualtrics) before you publish it on MTurk.
    
* Deploy your survey to MTurk.  We estimate that the survey will take about 5 minutes, and we would like to pay an hourly wage of $15 per hour so you should pay $1 per completed survey. ([How to link Qualtrics to MTurk](http://brentcurdy.net/qualtrics-tutorials/link/))
  
  - We have created four MTurk projects on our account with some pre-filled parameters, one for each group. Please fill in the other details. If you use your own account for data collection, we won't be able to reimburse you.

* After the data has been collected, remove any personally-identifying information from your responses.  Please ask us if you have any questions.

* Upload your data to this part of the Summer Institute's github repository: https://github.com/compsocialscience/summer-institute/tree/master/2017/materials/day4_surveys (if you need write access, let us know)

* Analyze your data. Compare your raw and weighted estimates, respectively, with what’s published by Pew. 

    - Get the Pew results to your questions. We will post them here: https://github.com/compsocialscience/summer-institute/tree/master/2017/materials/day4_surveys
    - Replicate Figure 1 from [Goel et al.](https://5harad.com/papers/dirtysurveys.pdf).
    - Next, try different adjustment methods, and for each method you use, replicate Figure 2 from [Goel et al.](https://5harad.com/papers/dirtysurveys.pdf). You will eventually present your figures to the entire class so that we can compare them.
    - When doing the adjustment methods, you should start with the simplest thing first.  Do not jump to the most fancy technique right away.  You may use packages, but only after you have coded up at least one technique by hand (if you have coded one of these techniques as part of your earlier research that counts).  

* Prepare your data for dissemination and open sourcing so that it can be used by others. ([Some best practices](https://www.icpsr.umich.edu/icpsrweb/content/deposit/guide/chapter3docs.html) for preparing survey metadata) 

* Combine the data collected by each group and repeat your analysis on the larger data set.

* Present results to class.

## Tips and additional resources

* You might want to divide the work among group members, including creating the survey on Qualtircs, reshaping data files, breaking up the analysis into modular functions, etc. 
* [Here](https://psrc.princeton.edu/our-services/using-mturk) are some more resources on web surveys  

## Further reading

Dutwin and Buskirk. ["Apples to Oranges or Gala versus Golden Delicious?: Comparing Data Quality of Nonprobability Internet Samples to Low Response Rate Probability Samples"](https://academic.oup.com/poq/article/81/S1/213/3749202/Apples-to-Oranges-or-Gala-versus-Golden-Delicious) 

Baker et al. ["Summary Report of the AAPOR Task Force on Non-probability Sampling"](https://academic.oup.com/jssam/article/1/2/90/941418/Summary-Report-of-the-AAPOR-Task-Force-on-Non)
